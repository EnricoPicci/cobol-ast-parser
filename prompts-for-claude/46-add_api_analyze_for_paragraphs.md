The following prompt has been generated by Claude while working on KYCo-demo project.
KYCo-demo project has a requirement for cobol-ast-parser and generated this prompt for Claude to implement it in this project.

# Prompt for cobol-ast-parser

## What follows is the prompt to use in the cobol-ast-parser project to implement the new API

---

## Task: Add `filter_data_division_by_paragraphs()` API

Add a new public function to `cobol_ast/api.py` that filters a COBOL program's DATA DIVISION to only the 01-level groups containing variables referenced by a given set of paragraphs.

### Why this is needed

A downstream project (KYCo) builds LLM prompts from COBOL execution traces. The prompt includes DATA DIVISION sections (WORKING-STORAGE + LINKAGE) for every executed module. For a typical execution with 26 modules, the full DATA DIVISION content is ~1.4 MB — too large for LLM context windows. We need to filter it down to only the variable groups actually referenced by the executed paragraphs.

### Function signature

```python
@dataclass
class FilterOptions:
    """Options for filtering DATA DIVISION by paragraph usage.

    Attributes:
        copybook_paths: Additional paths to search for copybooks.
        resolve_copies: Whether to resolve COPY statements (default: True).
        include_filler: Include FILLER items in output (default: True).
        include_88_levels: Include 88-level condition names in output (default: True).
    """
    copybook_paths: Optional[List[Path]] = None
    resolve_copies: bool = True
    include_filler: bool = True
    include_88_levels: bool = True


@dataclass
class FilteredDataDivisionResult:
    """Result of filtering DATA DIVISION by paragraph usage.

    Attributes:
        program_name: Name of the analyzed COBOL program.
        sections: Filtered DATA DIVISION sections (only groups with referenced variables).
        all_records: Flat list of all included Level 01 records.
        summary: Statistics (total/filtered record counts, reduction percentage).
        paragraph_names_used: The paragraph names that were matched.
        execution_time_seconds: Time taken for analysis + filtering.
        warnings: Warning messages from preprocessing.
    """
    program_name: str
    sections: List[DataDivisionSection]
    all_records: List[DataItemNode]
    summary: Dict[str, Any]
    paragraph_names_used: List[str]
    execution_time_seconds: float
    warnings: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        ...

    def to_text(self) -> str:
        """Render filtered data division as COBOL-like text for prompt inclusion."""
        ...


def filter_data_division_by_paragraphs(
    source_path: Path,
    paragraph_names: List[str],
    options: Optional[FilterOptions] = None,
) -> FilteredDataDivisionResult:
    """Filter DATA DIVISION to only groups containing variables referenced by given paragraphs.

    Uses analyze_with_tree() internally to get both the paragraph-variables mapping
    and the DATA DIVISION tree, then filters to only include 01-level groups where
    at least one variable (at any nesting level) is accessed or modified by any of
    the specified paragraphs.

    When a group is included, it is included in full (all children/siblings),
    preserving the complete structure for context.

    Args:
        source_path: Path to the COBOL source file.
        paragraph_names: List of paragraph/section names to filter by.
            These should match the paragraph names in the analysis output.
        options: Filter options (uses defaults if not provided).

    Returns:
        FilteredDataDivisionResult with only the relevant data groups.
    """
```

### Algorithm

1. Call `analyze_with_tree(source_path, ...)` to get `CombinedResult` with both the `data_division_tree` and the `analysis_result` (which contains `variable_index`)

2. Build a set of referenced `defined_in_record` names:
   - The `variable_index` has structure: `{defined_in_record: {"start:end": {"variable_name": str, "modifying_paragraphs": [...], "accessing_paragraphs": [...]}}}`
   - For each `defined_in_record` → for each position key → check if any paragraph in `modifying_paragraphs` or `accessing_paragraphs` is in the `paragraph_names` set
   - If yes, add that `defined_in_record` to the "needed records" set

3. Filter the `data_division_tree.sections`:
   - For each section, keep only the records (01-level `DataItemNode`) whose `name` (or `defined_in_record`) is in the "needed records" set
   - When a record is included, include it **in full** (all children at all levels)
   - Skip sections that end up with zero records

4. Build summary with: original record count, filtered record count, reduction percentage

5. The `to_text()` method should render the filtered tree as readable COBOL-like text suitable for LLM prompt inclusion:
   ```
   WORKING-STORAGE SECTION.

   01  WS-DEBUG-INDIC                    PIC X(01).
       88 WS-DO-DEBUG                    VALUE '1'.

   01  Y2K-DATE-AREA.
       05  CDY2-CURRENT-DATE             PIC X(21).
       05  CDY2-DATE-SSAA                PIC X(04).
       ...

   LINKAGE SECTION.

   01  LS-PARAMETER-AREA.
       05  LS-FUNCTION-CODE              PIC X(02).
       ...
   ```
   Format each item as: `{indent}{level:02d}  {name}{pic_clause}{value_clause}.` with appropriate indentation based on level number. Include 88-levels and FILLER items as configured.

### Existing code to reuse

- `analyze_with_tree()` in `api.py` already does the heavy lifting (parsing, analysis, tree building) in a single pass
- `CombinedResult.analysis_result.variable_index` provides the paragraph-to-variable mapping
- `DataDivisionTree.sections[].records[]` provides the tree to filter
- `DataItemNode` already has `defined_in_record`, `name`, `children`, `level`, `picture`, `usage`, `value`, etc.

### Paragraph name matching

The `variable_index` uses paragraph names as they appear in the analysis output (e.g., `"MAIN"`, `"OPERAZIONI-INIZIALI"`, `"ELABORA-HOST-ACTION"`). The caller passes the same names. Matching should be **case-insensitive** since COBOL is case-insensitive.

Some paragraphs in the variable_index may include the section name (e.g., `"MAIN SECTION"`). The matching should handle both with and without the `SECTION` suffix.

### Testing

Add tests in a new test file (or extend existing test files) covering:

1. **Basic filtering** — Given a program with 5 records and 2 paragraphs that reference variables in 2 of those records, only those 2 records are returned
2. **Full group inclusion** — When a child variable is referenced, the entire 01-level group is included with all siblings
3. **No matches** — When paragraph_names don't match any paragraphs, return empty sections
4. **All matches** — When all records have referenced variables, return the full tree
5. **Case insensitivity** — Paragraph names match regardless of case
6. **to_text() output** — Verify the text rendering produces valid COBOL-like structure
7. **Summary statistics** — Verify reduction percentages are calculated correctly

Use an existing test COBOL file from the test fixtures, or create a minimal one for these tests.

### Integration notes

The downstream project (KYCo) will call this function like:

```python
from cobol_ast.api import filter_data_division_by_paragraphs, FilterOptions

result = filter_data_division_by_paragraphs(
    source_path=Path("EGEGJQ00.cbl"),
    paragraph_names=["MAIN", "OPERAZIONI-INIZIALI", "ELABORA-HOST-ACTION"],
    options=FilterOptions(
        copybook_paths=[Path("./copybooks")],
    ),
)

# Get filtered text for prompt inclusion
filtered_text = result.to_text()

# Or get structured data
filtered_dict = result.to_dict()
```

### Important

- Follow the existing patterns in `api.py` for error handling (`AnalysisError`, `FileNotFoundError`, `ParseError`)
- Add the new function and dataclasses to `api.py` alongside the existing public API
- Export the new symbols from `__init__.py`
- The function should be documented with the same style as existing functions (docstrings with Args/Returns/Raises/Example sections) 

# Refinement
If filter_data_division_by_paragraphs is the only api that the client must call, then change its name to make it more clear what it does, which is more that filtering data division by paragraphs

# Response
the current implementation only returns filtered DATA DIVISION data. Here's what FilteredDataDivisionResult contains:

  ┌──────────────────────┬─────────────────────────────────────────────────────────────────────────────┐
  │        Field         │                                 What it is                                  │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ program_name         │ Program identifier                                                          │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ sections             │ Filtered DATA DIVISION sections (only records referenced by the paragraphs) │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ all_records          │ Flat list of included 01-level records                                      │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ summary              │ Filtering stats (before/after counts, reduction %)                          │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ paragraph_names_used │ Which paragraph names actually matched                                      │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ to_text()            │ COBOL-like text rendering for LLM prompts                                   │
  ├──────────────────────┼─────────────────────────────────────────────────────────────────────────────┤
  │ to_dict()            │ JSON-serializable dict                                                      │
  └──────────────────────┴─────────────────────────────────────────────────────────────────────────────┘

  The analysis, variable index, and paragraph-variables mapping are all computed internally but discarded — only the filtered tree
  comes out.

  So the question is: should this API also expose the richer data (the analysis result, variable index, etc.)? Or is the current
  scope correct for KYCo's use case — just the filtered DATA DIVISION text?

  # Refinement
  the API should return both the filtered DATA DIVISION and the analysis result (variable index, paragraph-variables 

  